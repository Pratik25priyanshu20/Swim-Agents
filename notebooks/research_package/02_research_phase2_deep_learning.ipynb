{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWIM Research — Phase 2: Deep Learning Baselines (GPU)\n",
    "\n",
    "**Builds on:** Phase 1 results (AUROC 0.814 ensemble baseline)\n",
    "\n",
    "### What this notebook does:\n",
    "1. Loads Phase 1 research dataset (train/val/test parquets)\n",
    "2. Creates temporal sequences (7-day sliding windows) for LSTM\n",
    "3. Trains LSTM baseline\n",
    "4. Trains lightweight Transformer baseline\n",
    "5. Trains XGBoost (proper gradient boosting baseline)\n",
    "6. Runs **modality ablation** (in-situ only, satellite only, all) — fixes Phase 1 gap\n",
    "7. Compares ALL models: GB, RF, Ensemble, XGBoost, LSTM, Transformer\n",
    "8. Runs **modality dropout experiment** (RQ1 core experiment)\n",
    "9. Exports everything as ZIP\n",
    "\n",
    "### Requires:\n",
    "- GPU (LSTM + Transformer training)\n",
    "- Phase 1 results (train/val/test parquets) OR the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 0. Setup ───\n",
    "import os, sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Install if needed\n",
    "# !pip install -q torch pandas numpy matplotlib seaborn scikit-learn xgboost pyarrow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import zipfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, brier_score_loss,\n",
    "                             roc_curve, precision_recall_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "    print('XGBoost available')\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "    print('XGBoost not installed, will skip. Install with: pip install xgboost')\n",
    "\n",
    "# GPU check\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'  GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'  Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "RESULTS_DIR = Path('results_phase2')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "(RESULTS_DIR / 'figures').mkdir(exist_ok=True)\n",
    "(RESULTS_DIR / 'models').mkdir(exist_ok=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Phase 1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading Phase 1 outputs, fall back to raw data\n",
    "phase1_dir = Path('results/data')\n",
    "if not phase1_dir.exists():\n",
    "    phase1_dir = Path('../results/data')\n",
    "\n",
    "if (phase1_dir / 'unified_research_dataset.parquet').exists():\n",
    "    print('Loading Phase 1 outputs...')\n",
    "    research_df = pd.read_parquet(phase1_dir / 'unified_research_dataset.parquet')\n",
    "    print(f'Loaded: {len(research_df):,} rows, {len(research_df.columns)} cols')\n",
    "else:\n",
    "    print('Phase 1 outputs not found. Run Phase 1 notebook first.')\n",
    "    print(f'Looked in: {phase1_dir.absolute()}')\n",
    "    raise FileNotFoundError('Run Phase 1 first')\n",
    "\n",
    "print(f'Lakes: {research_df[\"lake\"].value_counts().to_dict()}')\n",
    "print(f'Date range: {research_df[\"date\"].min()} → {research_df[\"date\"].max()}')\n",
    "print(f'Bloom rate: {research_df[\"bloom_label\"].mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Define feature sets ───\n",
    "INSITU_FEATURES = ['chlorophyll_a', 'turbidity', 'dissolved_oxygen', 'ph',\n",
    "                   'temperature', 'conductivity', 'wind_speed', 'air_temperature', 'humidity']\n",
    "\n",
    "SATELLITE_FEATURES = ['ndvi', 'surface_temperature', 'chlorophyll_index',\n",
    "                      'turbidity_index', 'cloud_coverage']\n",
    "\n",
    "ALL_FEATURES = [f for f in INSITU_FEATURES + SATELLITE_FEATURES if f in research_df.columns]\n",
    "TARGET = 'bloom_label'\n",
    "\n",
    "print(f'Features ({len(ALL_FEATURES)}): {ALL_FEATURES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Stratified split (same as Phase 1 fallback) ───\n",
    "X_all = research_df[ALL_FEATURES].values\n",
    "y_all = research_df[TARGET].values\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_all_imp = imputer.fit_transform(X_all)\n",
    "X_all_scaled = scaler.fit_transform(X_all_imp)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_all_scaled, y_all, test_size=0.3, random_state=42, stratify=y_all)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f'Train: {X_train.shape} | bloom={y_train.mean():.3f}')\n",
    "print(f'Val:   {X_val.shape} | bloom={y_val.mean():.3f}')\n",
    "print(f'Test:  {X_test.shape} | bloom={y_test.mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Create Temporal Sequences for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Build sequences per lake (7-day sliding window) ───\n",
    "SEQ_LEN = 7\n",
    "\n",
    "def create_sequences(df, features, target, seq_len, scaler_fit=None):\n",
    "    \"\"\"Create sliding window sequences per lake.\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    imp = SimpleImputer(strategy='median')\n",
    "    sc = scaler_fit if scaler_fit else StandardScaler()\n",
    "    \n",
    "    df_sorted = df.sort_values(['lake', 'date']).copy()\n",
    "    X_raw = df_sorted[features].values\n",
    "    \n",
    "    if scaler_fit is None:\n",
    "        X_proc = sc.fit_transform(imp.fit_transform(X_raw))\n",
    "    else:\n",
    "        X_proc = sc.transform(imp.transform(X_raw))\n",
    "    \n",
    "    y_raw = df_sorted[target].values\n",
    "    lake_ids = df_sorted['lake'].values\n",
    "    \n",
    "    for i in range(seq_len, len(X_proc)):\n",
    "        # Only create sequence if all days are from the same lake\n",
    "        if len(set(lake_ids[i-seq_len:i+1])) == 1:\n",
    "            sequences.append(X_proc[i-seq_len:i])\n",
    "            labels.append(y_raw[i])\n",
    "    \n",
    "    return np.array(sequences), np.array(labels), sc, imp\n",
    "\n",
    "# Split data by time first, then create sequences\n",
    "df_sorted = research_df.sort_values('date').reset_index(drop=True)\n",
    "n = len(df_sorted)\n",
    "train_df = df_sorted.iloc[:int(n*0.7)]\n",
    "val_df = df_sorted.iloc[int(n*0.7):int(n*0.85)]\n",
    "test_df = df_sorted.iloc[int(n*0.85):]\n",
    "\n",
    "X_seq_train, y_seq_train, seq_scaler, seq_imputer = create_sequences(\n",
    "    train_df, ALL_FEATURES, TARGET, SEQ_LEN)\n",
    "X_seq_val, y_seq_val, _, _ = create_sequences(\n",
    "    val_df, ALL_FEATURES, TARGET, SEQ_LEN, scaler_fit=seq_scaler)\n",
    "X_seq_test, y_seq_test, _, _ = create_sequences(\n",
    "    test_df, ALL_FEATURES, TARGET, SEQ_LEN, scaler_fit=seq_scaler)\n",
    "\n",
    "print(f'Sequence length: {SEQ_LEN} days')\n",
    "print(f'Train sequences: {X_seq_train.shape} | bloom={y_seq_train.mean():.3f}')\n",
    "print(f'Val sequences:   {X_seq_val.shape} | bloom={y_seq_val.mean():.3f}')\n",
    "print(f'Test sequences:  {X_seq_test.shape} | bloom={y_seq_test.mean():.3f}')\n",
    "\n",
    "# Check if we have enough sequences with both classes\n",
    "has_seq_both_train = len(np.unique(y_seq_train)) > 1\n",
    "has_seq_both_test = len(np.unique(y_seq_test)) > 1\n",
    "print(f'\\nTrain has both classes: {has_seq_both_train}')\n",
    "print(f'Test has both classes: {has_seq_both_test}')\n",
    "\n",
    "if not has_seq_both_train or not has_seq_both_test:\n",
    "    print('\\n  Temporal sequences lack both classes. Using stratified approach for LSTM too.')\n",
    "    # Create sequences from full data, then stratified split\n",
    "    X_seq_all, y_seq_all, seq_scaler, seq_imputer = create_sequences(\n",
    "        research_df, ALL_FEATURES, TARGET, SEQ_LEN)\n",
    "    \n",
    "    X_seq_train, X_seq_temp, y_seq_train, y_seq_temp = train_test_split(\n",
    "        X_seq_all, y_seq_all, test_size=0.3, random_state=42, stratify=y_seq_all)\n",
    "    X_seq_val, X_seq_test, y_seq_val, y_seq_test = train_test_split(\n",
    "        X_seq_temp, y_seq_temp, test_size=0.5, random_state=42, stratify=y_seq_temp)\n",
    "    \n",
    "    print(f'  Stratified Train: {X_seq_train.shape} | bloom={y_seq_train.mean():.3f}')\n",
    "    print(f'  Stratified Val:   {X_seq_val.shape} | bloom={y_seq_val.mean():.3f}')\n",
    "    print(f'  Stratified Test:  {X_seq_test.shape} | bloom={y_seq_test.mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 3A: LSTM Architecture ───\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers,\n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        last_hidden = lstm_out[:, -1, :]  # last timestep\n",
    "        out = self.dropout(last_hidden)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "print(f'LSTM: input_dim={len(ALL_FEATURES)}, hidden=64, layers=2, dropout=0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 3B: Train LSTM ───\n",
    "def train_torch_model(model, X_train, y_train, X_val, y_val,\n",
    "                      epochs=100, batch_size=64, lr=0.001, patience=15):\n",
    "    \"\"\"Train a PyTorch model with early stopping.\"\"\"\n",
    "    train_ds = TensorDataset(\n",
    "        torch.FloatTensor(X_train).to(device),\n",
    "        torch.FloatTensor(y_train).to(device))\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_t = torch.FloatTensor(y_val).to(device)\n",
    "    \n",
    "    # Class weight for imbalanced data\n",
    "    pos_weight = torch.tensor([(1 - y_train.mean()) / max(y_train.mean(), 0.01)]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    wait = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_auroc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(X_val_t)\n",
    "            val_loss = criterion(val_logits, y_val_t).item()\n",
    "            val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "            try:\n",
    "                val_auroc = roc_auc_score(y_val, val_probs)\n",
    "            except ValueError:\n",
    "                val_auroc = 0.5\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        history['train_loss'].append(np.mean(train_losses))\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_auroc'].append(val_auroc)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'  Epoch {epoch+1:3d} | train_loss={np.mean(train_losses):.4f} | '\n",
    "                  f'val_loss={val_loss:.4f} | val_auroc={val_auroc:.4f}')\n",
    "        \n",
    "        if wait >= patience:\n",
    "            print(f'  Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return model, history\n",
    "\n",
    "# Train LSTM\n",
    "print('Training LSTM...')\n",
    "lstm_model = LSTMClassifier(input_dim=len(ALL_FEATURES)).to(device)\n",
    "lstm_model, lstm_history = train_torch_model(\n",
    "    lstm_model, X_seq_train, y_seq_train, X_seq_val, y_seq_val,\n",
    "    epochs=100, batch_size=64, lr=0.001, patience=15)\n",
    "print('LSTM training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 4A: Lightweight Transformer ───\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 100, d_model) * 0.1)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=128,\n",
    "            dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(d_model, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # global average pooling\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "print(f'Transformer: d_model=64, heads=4, layers=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 4B: Train Transformer ───\n",
    "print('Training Transformer...')\n",
    "transformer_model = TransformerClassifier(input_dim=len(ALL_FEATURES)).to(device)\n",
    "transformer_model, transformer_history = train_torch_model(\n",
    "    transformer_model, X_seq_train, y_seq_train, X_seq_val, y_seq_val,\n",
    "    epochs=100, batch_size=64, lr=0.0005, patience=15)\n",
    "print('Transformer training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Tabular Baselines (XGBoost + Phase 1 models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 5A: Train all tabular models ───\n",
    "tabular_models = OrderedDict()\n",
    "\n",
    "tabular_models['GradientBoosting'] = GradientBoostingClassifier(\n",
    "    n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "    subsample=0.8, min_samples_leaf=10, random_state=42)\n",
    "\n",
    "tabular_models['RandomForest'] = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=10, min_samples_leaf=5,\n",
    "    random_state=42, n_jobs=-1)\n",
    "\n",
    "if HAS_XGB:\n",
    "    tabular_models['XGBoost'] = xgb.XGBClassifier(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        scale_pos_weight=(1 - y_train.mean()) / max(y_train.mean(), 0.01),\n",
    "        random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "for name, model in tabular_models.items():\n",
    "    t0 = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    elapsed = (datetime.now() - t0).total_seconds()\n",
    "    print(f'  {name}: trained in {elapsed:.1f}s')\n",
    "\n",
    "print('All tabular models trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Unified Evaluation — ALL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6A: Evaluation function ───\n",
    "def evaluate(name, y_true, y_prob):\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    has_both = len(np.unique(y_true)) > 1\n",
    "    return {\n",
    "        'model': name,\n",
    "        'AUROC': roc_auc_score(y_true, y_prob) if has_both else np.nan,\n",
    "        'AUPRC': average_precision_score(y_true, y_prob) if has_both else np.nan,\n",
    "        'Brier': brier_score_loss(y_true, y_prob),\n",
    "        'Accuracy': (y_pred == y_true).mean(),\n",
    "        'TP': int(((y_pred == 1) & (y_true == 1)).sum()),\n",
    "        'FP': int(((y_pred == 1) & (y_true == 0)).sum()),\n",
    "        'FN': int(((y_pred == 0) & (y_true == 1)).sum()),\n",
    "        'TN': int(((y_pred == 0) & (y_true == 0)).sum()),\n",
    "    }\n",
    "\n",
    "def get_torch_probs(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.FloatTensor(X).to(device))\n",
    "        return torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "# Collect all predictions\n",
    "all_results = []\n",
    "\n",
    "# Tabular models (on flat features)\n",
    "ens_prob = np.zeros(len(y_test))\n",
    "for name, model in tabular_models.items():\n",
    "    prob = model.predict_proba(X_test)[:, 1]\n",
    "    ens_prob += prob / len(tabular_models)\n",
    "    all_results.append(evaluate(name, y_test, prob))\n",
    "\n",
    "all_results.append(evaluate('Tabular Ensemble', y_test, ens_prob))\n",
    "\n",
    "# LSTM (on sequences)\n",
    "lstm_prob = get_torch_probs(lstm_model, X_seq_test)\n",
    "all_results.append(evaluate('LSTM', y_seq_test, lstm_prob))\n",
    "\n",
    "# Transformer (on sequences)\n",
    "transformer_prob = get_torch_probs(transformer_model, X_seq_test)\n",
    "all_results.append(evaluate('Transformer', y_seq_test, transformer_prob))\n",
    "\n",
    "# DL Ensemble\n",
    "dl_ens_prob = (lstm_prob + transformer_prob) / 2\n",
    "all_results.append(evaluate('DL Ensemble (LSTM+Trans)', y_seq_test, dl_ens_prob))\n",
    "\n",
    "# Display\n",
    "print('ALL MODELS — TEST SET RESULTS')\n",
    "print('=' * 90)\n",
    "results_df = pd.DataFrame(all_results)\n",
    "display(results_df.sort_values('AUROC', ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6B: Training curves (LSTM & Transformer) ───\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "for hist, name, color in [(lstm_history, 'LSTM', 'steelblue'),\n",
    "                           (transformer_history, 'Transformer', 'coral')]:\n",
    "    axes[0].plot(hist['train_loss'], label=f'{name} train', color=color, linewidth=2)\n",
    "    axes[0].plot(hist['val_loss'], '--', label=f'{name} val', color=color, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "for hist, name, color in [(lstm_history, 'LSTM', 'steelblue'),\n",
    "                           (transformer_history, 'Transformer', 'coral')]:\n",
    "    axes[1].plot(hist['val_auroc'], label=name, color=color, linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('AUROC')\n",
    "axes[1].set_title('Validation AUROC', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# Model comparison bar chart\n",
    "plot_df = results_df.dropna(subset=['AUROC']).sort_values('AUROC', ascending=True)\n",
    "colors = ['steelblue' if 'LSTM' not in m and 'Trans' not in m else 'coral'\n",
    "          for m in plot_df['model']]\n",
    "axes[2].barh(plot_df['model'], plot_df['AUROC'], color=colors, edgecolor='white')\n",
    "axes[2].set_xlabel('AUROC')\n",
    "axes[2].set_title('Model Comparison (Test AUROC)', fontweight='bold')\n",
    "axes[2].axvline(0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "for i, (_, row) in enumerate(plot_df.iterrows()):\n",
    "    axes[2].text(row['AUROC'] + 0.005, i, f'{row[\"AUROC\"]:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Phase 2: Deep Learning Training & Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'fig1_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6C: ROC + PR curves for all models ───\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Collect all (name, y_true, y_prob) pairs\n",
    "model_preds = []\n",
    "for name, model in tabular_models.items():\n",
    "    model_preds.append((name, y_test, model.predict_proba(X_test)[:, 1]))\n",
    "model_preds.append(('LSTM', y_seq_test, lstm_prob))\n",
    "model_preds.append(('Transformer', y_seq_test, transformer_prob))\n",
    "\n",
    "# ROC\n",
    "for name, yt, yp in model_preds:\n",
    "    if len(np.unique(yt)) > 1:\n",
    "        fpr, tpr, _ = roc_curve(yt, yp)\n",
    "        auc = roc_auc_score(yt, yp)\n",
    "        ls = '--' if name in ['LSTM', 'Transformer'] else '-'\n",
    "        axes[0].plot(fpr, tpr, ls, label=f'{name} ({auc:.3f})', linewidth=2)\n",
    "axes[0].plot([0, 1], [0, 1], 'k:', alpha=0.3)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curves — All Models', fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# PR\n",
    "for name, yt, yp in model_preds:\n",
    "    if len(np.unique(yt)) > 1:\n",
    "        prec, rec, _ = precision_recall_curve(yt, yp)\n",
    "        ap = average_precision_score(yt, yp)\n",
    "        ls = '--' if name in ['LSTM', 'Transformer'] else '-'\n",
    "        axes[1].plot(rec, prec, ls, label=f'{name} ({ap:.3f})', linewidth=2)\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('PR Curves — All Models', fontweight='bold')\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Phase 2: Full Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'fig2_roc_pr_all_models.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Modality Ablation (RQ1 Core Experiment)\n",
    "What happens when we **remove** satellite OR in-situ features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 7A: Ablation across all model types ───\n",
    "insitu_avail = [f for f in INSITU_FEATURES if f in ALL_FEATURES]\n",
    "sat_avail = [f for f in SATELLITE_FEATURES if f in ALL_FEATURES]\n",
    "\n",
    "feature_configs = {\n",
    "    'All Features': ALL_FEATURES,\n",
    "    'In-Situ Only': insitu_avail,\n",
    "    'Satellite Only': sat_avail,\n",
    "}\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "for config_name, feats in feature_configs.items():\n",
    "    if len(feats) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Get feature indices\n",
    "    feat_idx = [ALL_FEATURES.index(f) for f in feats]\n",
    "    \n",
    "    # Tabular models\n",
    "    X_tr_sub = X_train[:, feat_idx]\n",
    "    X_te_sub = X_test[:, feat_idx]\n",
    "    \n",
    "    gb_abl = GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "                                        subsample=0.8, random_state=42)\n",
    "    gb_abl.fit(X_tr_sub, y_train)\n",
    "    prob_gb = gb_abl.predict_proba(X_te_sub)[:, 1]\n",
    "    m = evaluate(f'GB — {config_name}', y_test, prob_gb)\n",
    "    m['config'] = config_name\n",
    "    m['model_type'] = 'GradientBoosting'\n",
    "    m['n_features'] = len(feats)\n",
    "    ablation_results.append(m)\n",
    "    \n",
    "    if HAS_XGB:\n",
    "        xgb_abl = xgb.XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "                                     subsample=0.8, random_state=42,\n",
    "                                     use_label_encoder=False, eval_metric='logloss')\n",
    "        xgb_abl.fit(X_tr_sub, y_train)\n",
    "        prob_xgb = xgb_abl.predict_proba(X_te_sub)[:, 1]\n",
    "        m2 = evaluate(f'XGB — {config_name}', y_test, prob_xgb)\n",
    "        m2['config'] = config_name\n",
    "        m2['model_type'] = 'XGBoost'\n",
    "        m2['n_features'] = len(feats)\n",
    "        ablation_results.append(m2)\n",
    "    \n",
    "    # LSTM ablation\n",
    "    X_seq_tr_sub = X_seq_train[:, :, feat_idx]\n",
    "    X_seq_val_sub = X_seq_val[:, :, feat_idx]\n",
    "    X_seq_te_sub = X_seq_test[:, :, feat_idx]\n",
    "    \n",
    "    lstm_abl = LSTMClassifier(input_dim=len(feats)).to(device)\n",
    "    lstm_abl, _ = train_torch_model(\n",
    "        lstm_abl, X_seq_tr_sub, y_seq_train, X_seq_val_sub, y_seq_val,\n",
    "        epochs=60, batch_size=64, lr=0.001, patience=10)\n",
    "    prob_lstm = get_torch_probs(lstm_abl, X_seq_te_sub)\n",
    "    m3 = evaluate(f'LSTM — {config_name}', y_seq_test, prob_lstm)\n",
    "    m3['config'] = config_name\n",
    "    m3['model_type'] = 'LSTM'\n",
    "    m3['n_features'] = len(feats)\n",
    "    ablation_results.append(m3)\n",
    "    \n",
    "    print(f'  {config_name} ({len(feats)} feats): GB AUROC={m[\"AUROC\"]:.3f} | LSTM AUROC={m3[\"AUROC\"]:.3f}')\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_results)\n",
    "print('\\nFull ablation results:')\n",
    "display(ablation_df[['model', 'config', 'model_type', 'n_features', 'AUROC', 'AUPRC', 'Brier', 'Accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 7B: Ablation visualization ───\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Grouped bar chart: AUROC by model type and feature config\n",
    "pivot = ablation_df.pivot_table(index='config', columns='model_type', values='AUROC')\n",
    "pivot = pivot.reindex(['All Features', 'In-Situ Only', 'Satellite Only'])\n",
    "pivot.plot.bar(ax=axes[0], edgecolor='white', width=0.7)\n",
    "axes[0].set_title('AUROC by Feature Set & Model Type', fontweight='bold')\n",
    "axes[0].set_ylabel('AUROC')\n",
    "axes[0].set_xlabel('Feature Configuration')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "axes[0].axhline(0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "axes[0].legend(title='Model')\n",
    "\n",
    "# Degradation plot: how much does each model lose when dropping modalities?\n",
    "deg_data = []\n",
    "for model_type in ablation_df['model_type'].unique():\n",
    "    sub = ablation_df[ablation_df['model_type'] == model_type]\n",
    "    all_auroc = sub[sub['config'] == 'All Features']['AUROC'].values\n",
    "    if len(all_auroc) == 0:\n",
    "        continue\n",
    "    all_auroc = all_auroc[0]\n",
    "    for _, row in sub.iterrows():\n",
    "        if pd.notna(row['AUROC']):\n",
    "            deg_data.append({\n",
    "                'model': model_type,\n",
    "                'config': row['config'],\n",
    "                'degradation': (all_auroc - row['AUROC']) / max(all_auroc, 0.01) * 100\n",
    "            })\n",
    "\n",
    "deg_df = pd.DataFrame(deg_data)\n",
    "deg_pivot = deg_df.pivot_table(index='config', columns='model', values='degradation')\n",
    "deg_pivot = deg_pivot.reindex(['All Features', 'In-Situ Only', 'Satellite Only'])\n",
    "deg_pivot.plot.bar(ax=axes[1], edgecolor='white', width=0.7)\n",
    "axes[1].set_title('AUROC Degradation When Dropping Modality (%)', fontweight='bold')\n",
    "axes[1].set_ylabel('% Degradation from All Features')\n",
    "axes[1].set_xlabel('Feature Configuration')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "axes[1].axhline(0, color='gray', linestyle='-', alpha=0.3)\n",
    "axes[1].legend(title='Model')\n",
    "\n",
    "plt.suptitle('RQ1: Modality Ablation — How Models Handle Missing Data Sources',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'fig3_modality_ablation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey insight for RQ1:')\n",
    "print('Compare these degradation patterns to how SWIM agents handle the same dropout.')\n",
    "print('If agents degrade less, that supports the agentic architecture thesis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Random Feature Dropout Experiment (RQ1)\n",
    "Simulate real-world sensor failures: randomly drop features at test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 8A: Feature dropout at varying rates ───\n",
    "dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "n_trials = 10  # average over random trials\n",
    "\n",
    "dropout_results = []\n",
    "\n",
    "for drop_rate in dropout_rates:\n",
    "    for trial in range(n_trials):\n",
    "        # Create dropout mask\n",
    "        mask = np.random.random(X_test.shape) > drop_rate\n",
    "        X_test_dropped = X_test * mask  # zero out dropped features\n",
    "        \n",
    "        # GB\n",
    "        prob_gb = tabular_models['GradientBoosting'].predict_proba(X_test_dropped)[:, 1]\n",
    "        m_gb = evaluate('GB', y_test, prob_gb)\n",
    "        dropout_results.append({'model': 'GradientBoosting', 'drop_rate': drop_rate,\n",
    "                                'trial': trial, 'AUROC': m_gb['AUROC']})\n",
    "        \n",
    "        # RF\n",
    "        prob_rf = tabular_models['RandomForest'].predict_proba(X_test_dropped)[:, 1]\n",
    "        m_rf = evaluate('RF', y_test, prob_rf)\n",
    "        dropout_results.append({'model': 'RandomForest', 'drop_rate': drop_rate,\n",
    "                                'trial': trial, 'AUROC': m_rf['AUROC']})\n",
    "        \n",
    "        if HAS_XGB:\n",
    "            prob_xgb = tabular_models['XGBoost'].predict_proba(X_test_dropped)[:, 1]\n",
    "            m_xgb = evaluate('XGB', y_test, prob_xgb)\n",
    "            dropout_results.append({'model': 'XGBoost', 'drop_rate': drop_rate,\n",
    "                                    'trial': trial, 'AUROC': m_xgb['AUROC']})\n",
    "        \n",
    "        # LSTM dropout\n",
    "        seq_mask = np.random.random(X_seq_test.shape) > drop_rate\n",
    "        X_seq_dropped = X_seq_test * seq_mask\n",
    "        prob_lstm = get_torch_probs(lstm_model, X_seq_dropped)\n",
    "        m_lstm = evaluate('LSTM', y_seq_test, prob_lstm)\n",
    "        dropout_results.append({'model': 'LSTM', 'drop_rate': drop_rate,\n",
    "                                'trial': trial, 'AUROC': m_lstm['AUROC']})\n",
    "\n",
    "dropout_df = pd.DataFrame(dropout_results)\n",
    "print(f'Dropout experiment: {len(dropout_df)} evaluations')\n",
    "print(dropout_df.groupby(['model', 'drop_rate'])['AUROC'].mean().unstack().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 8B: Dropout degradation curves ───\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# AUROC vs dropout rate\n",
    "for model_name in dropout_df['model'].unique():\n",
    "    sub = dropout_df[dropout_df['model'] == model_name]\n",
    "    means = sub.groupby('drop_rate')['AUROC'].mean()\n",
    "    stds = sub.groupby('drop_rate')['AUROC'].std()\n",
    "    axes[0].plot(means.index, means.values, 'o-', label=model_name, linewidth=2, markersize=6)\n",
    "    axes[0].fill_between(means.index, means - stds, means + stds, alpha=0.15)\n",
    "\n",
    "axes[0].set_xlabel('Feature Dropout Rate')\n",
    "axes[0].set_ylabel('AUROC')\n",
    "axes[0].set_title('Model Robustness Under Feature Dropout', fontweight='bold')\n",
    "axes[0].axhline(0.5, color='gray', linestyle=':', alpha=0.5, label='Random')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(-0.02, 0.72)\n",
    "\n",
    "# Relative degradation\n",
    "for model_name in dropout_df['model'].unique():\n",
    "    sub = dropout_df[dropout_df['model'] == model_name]\n",
    "    means = sub.groupby('drop_rate')['AUROC'].mean()\n",
    "    baseline = means.iloc[0]  # no dropout\n",
    "    relative = (means - baseline) / max(baseline, 0.01) * 100\n",
    "    axes[1].plot(relative.index, relative.values, 'o-', label=model_name, linewidth=2, markersize=6)\n",
    "\n",
    "axes[1].set_xlabel('Feature Dropout Rate')\n",
    "axes[1].set_ylabel('AUROC Change (%)')\n",
    "axes[1].set_title('Relative AUROC Degradation', fontweight='bold')\n",
    "axes[1].axhline(0, color='gray', linestyle='-', alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(-0.02, 0.72)\n",
    "\n",
    "plt.suptitle('RQ1: Feature Dropout Robustness (Monolithic Models)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'fig4_feature_dropout.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nThis is the MONOLITHIC baseline for RQ1.')\n",
    "print('In Phase 3, we compare these curves to the SWIM agentic architecture.')\n",
    "print('If agents maintain higher AUROC under dropout → paper contribution.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Save Everything & Export ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 9A: Save all results ───\n",
    "results_df.to_csv(RESULTS_DIR / 'all_models_test_results.csv', index=False)\n",
    "ablation_df.to_csv(RESULTS_DIR / 'ablation_results.csv', index=False)\n",
    "dropout_df.to_csv(RESULTS_DIR / 'dropout_experiment.csv', index=False)\n",
    "\n",
    "# Save models\n",
    "torch.save(lstm_model.state_dict(), RESULTS_DIR / 'models' / 'lstm_model.pt')\n",
    "torch.save(transformer_model.state_dict(), RESULTS_DIR / 'models' / 'transformer_model.pt')\n",
    "for name, model in tabular_models.items():\n",
    "    with open(RESULTS_DIR / 'models' / f'{name.lower()}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Save histories\n",
    "with open(RESULTS_DIR / 'lstm_history.json', 'w') as f:\n",
    "    json.dump(lstm_history, f)\n",
    "with open(RESULTS_DIR / 'transformer_history.json', 'w') as f:\n",
    "    json.dump(transformer_history, f)\n",
    "\n",
    "# Experiment metadata\n",
    "meta = {\n",
    "    'experiment': 'RQ1_Phase2_Deep_Learning_Baselines',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'device': str(device),\n",
    "    'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "    'dataset': {\n",
    "        'total': len(research_df),\n",
    "        'train_flat': len(y_train),\n",
    "        'test_flat': len(y_test),\n",
    "        'train_seq': len(y_seq_train),\n",
    "        'test_seq': len(y_seq_test),\n",
    "        'seq_len': SEQ_LEN,\n",
    "        'n_features': len(ALL_FEATURES),\n",
    "        'features': ALL_FEATURES,\n",
    "    },\n",
    "    'models': {\n",
    "        'LSTM': {'hidden': 64, 'layers': 2, 'dropout': 0.3},\n",
    "        'Transformer': {'d_model': 64, 'heads': 4, 'layers': 2, 'dropout': 0.3},\n",
    "        'GradientBoosting': {'n_estimators': 200, 'max_depth': 5},\n",
    "        'RandomForest': {'n_estimators': 200, 'max_depth': 10},\n",
    "    },\n",
    "    'test_results': results_df.to_dict(orient='records'),\n",
    "    'ablation_summary': ablation_df[['model', 'config', 'AUROC', 'AUPRC']].to_dict(orient='records'),\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'experiment_metadata.json', 'w') as f:\n",
    "    json.dump(meta, f, indent=2, default=str)\n",
    "\n",
    "print('All results saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 9B: Create ZIP ───\n",
    "zip_path = 'swim_research_phase2_results.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    for root, dirs, files in os.walk(RESULTS_DIR):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, '.')\n",
    "            zf.write(file_path, arcname)\n",
    "\n",
    "zip_size = os.path.getsize(zip_path) / (1024 * 1024)\n",
    "print(f'\\nResults ZIP: {zip_path} ({zip_size:.1f} MB)')\n",
    "print('\\nContents:')\n",
    "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "    for info in zf.infolist():\n",
    "        print(f'  {info.filename:<55s} {info.file_size/1024:>8.1f} KB')\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(zip_path)\n",
    "else:\n",
    "    print(f'\\nDownload from: {os.path.abspath(zip_path)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Models Trained:\n",
    "| Model | Type | Data Format |\n",
    "|-------|------|-------------|\n",
    "| GradientBoosting | Tabular | Flat features |\n",
    "| RandomForest | Tabular | Flat features |\n",
    "| XGBoost | Tabular | Flat features |\n",
    "| LSTM (2-layer) | Sequence | 7-day windows |\n",
    "| Transformer | Sequence | 7-day windows |\n",
    "\n",
    "### Experiments Run:\n",
    "1. Full model comparison (AUROC, AUPRC, Brier, Accuracy)\n",
    "2. Modality ablation (in-situ only, satellite only, all)\n",
    "3. Feature dropout robustness (0-70% random dropout, 10 trials each)\n",
    "\n",
    "### Next Steps:\n",
    "- **Phase 3**: Run same data through SWIM agents, compare dropout curves\n",
    "- **Phase 4**: Communication protocol experiments (RQ2)\n",
    "- **Phase 5**: Conflict resolution experiments (RQ3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}