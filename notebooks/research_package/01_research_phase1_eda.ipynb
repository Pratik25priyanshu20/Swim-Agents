{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWIM Research — Phase 1: Data Audit, EDA & Monolithic Baselines\n",
    "\n",
    "**Project:** Agentic AI for Multi-Modal Earth Observation  \n",
    "**RQ1:** Does decomposing environmental prediction into specialized agents outperform monolithic models?  \n",
    "\n",
    "---\n",
    "\n",
    "### What this notebook does:\n",
    "1. Loads & audits all 4 data sources (53K ml_ready + 49K in-situ + 8.7K satellite + lake JSON bloom labels)\n",
    "2. Checks data quality — flags synthetic data, missing values, suspicious patterns\n",
    "3. Builds a unified research dataset merging all sources\n",
    "4. Creates proper bloom labels (multi-criteria)\n",
    "5. Temporal train/val/test split (no data leakage)\n",
    "6. Trains monolithic baselines (GradientBoosting, RandomForest, Ensemble)\n",
    "7. Evaluates with research metrics (AUROC, AUPRC, Brier, ECE)\n",
    "8. Exports everything as a zip\n",
    "\n",
    "### Data included in this package:\n",
    "```\n",
    "data/\n",
    "  processed/ml_ready_data.csv          (53K rows, 10 features)\n",
    "  raw/in_situ/LUBW_BW_...csv           (49K rows, 23 features)\n",
    "  raw/satellite/*.csv                   (8.7K rows, spectral indices)\n",
    "  raw/lake_data/*.json                  (bloom probability, status, toxin levels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup & Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment (Colab vs local Jupyter)\n",
    "import os, sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('Running in Google Colab')\n",
    "    # If running in Colab, upload the zip or mount Google Drive\n",
    "    # Option A: Upload zip manually\n",
    "    # from google.colab import files\n",
    "    # uploaded = files.upload()  # Upload research_data.zip\n",
    "    # !unzip research_data.zip -d /content/research_data\n",
    "    # DATA_ROOT = '/content/research_data/data'\n",
    "    \n",
    "    # Option B: Mount Google Drive (if you uploaded data there)\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    # DATA_ROOT = '/content/drive/MyDrive/SWIM_Research/data'\n",
    "    \n",
    "    # Default: assume zip was extracted to /content/data\n",
    "    DATA_ROOT = '/content/data'\n",
    "    print(f'  Data root: {DATA_ROOT}')\n",
    "else:\n",
    "    print('Running in local Jupyter')\n",
    "    # Data is right next to this notebook\n",
    "    DATA_ROOT = os.path.join(os.path.dirname(os.path.abspath('.')), 'research_package', 'data')\n",
    "    # Fallback: try relative path\n",
    "    if not os.path.exists(DATA_ROOT):\n",
    "        DATA_ROOT = './data'\n",
    "    if not os.path.exists(DATA_ROOT):\n",
    "        DATA_ROOT = '../research_package/data'\n",
    "    print(f'  Data root: {os.path.abspath(DATA_ROOT)}')\n",
    "\n",
    "# Verify data exists\n",
    "required = [\n",
    "    os.path.join(DATA_ROOT, 'processed', 'ml_ready_data.csv'),\n",
    "    os.path.join(DATA_ROOT, 'raw', 'in_situ'),\n",
    "    os.path.join(DATA_ROOT, 'raw', 'satellite'),\n",
    "    os.path.join(DATA_ROOT, 'raw', 'lake_data'),\n",
    "]\n",
    "for p in required:\n",
    "    exists = os.path.exists(p)\n",
    "    print(f'  {\"OK\" if exists else \"MISSING\"}: {p}')\n",
    "    if not exists:\n",
    "        print(f'    -> Fix DATA_ROOT above or upload data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab may need these)\n",
    "# !pip install -q pandas numpy matplotlib seaborn scikit-learn pyarrow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import shutil\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, brier_score_loss,\n",
    "                             classification_report, confusion_matrix, \n",
    "                             roc_curve, precision_recall_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Output directory for all results\n",
    "RESULTS_DIR = Path('results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "(RESULTS_DIR / 'figures').mkdir(exist_ok=True)\n",
    "(RESULTS_DIR / 'models').mkdir(exist_ok=True)\n",
    "(RESULTS_DIR / 'data').mkdir(exist_ok=True)\n",
    "\n",
    "DATA_ROOT = Path(DATA_ROOT)\n",
    "print(f'Results will be saved to: {RESULTS_DIR.absolute()}')\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1A: ML-ready data (pre-processed, 10 features) ───\n",
    "ml_data = pd.read_csv(DATA_ROOT / 'processed' / 'ml_ready_data.csv', parse_dates=['timestamp'])\n",
    "print(f'1. ml_ready_data: {ml_data.shape[0]:,} rows x {ml_data.shape[1]} cols')\n",
    "print(f'   Date range: {ml_data[\"timestamp\"].min()} → {ml_data[\"timestamp\"].max()}')\n",
    "print(f'   Lakes: {ml_data[\"lake\"].nunique()} unique')\n",
    "print(f'   Top lakes: {ml_data[\"lake\"].value_counts().head(5).to_dict()}')\n",
    "display(ml_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1B: In-situ measurements (raw, 23 features) ───\n",
    "insitu_path = list((DATA_ROOT / 'raw' / 'in_situ').glob('*.csv'))[0]\n",
    "insitu = pd.read_csv(insitu_path, parse_dates=['timestamp'])\n",
    "print(f'2. in_situ: {insitu.shape[0]:,} rows x {insitu.shape[1]} cols')\n",
    "print(f'   Date range: {insitu[\"timestamp\"].min()} → {insitu[\"timestamp\"].max()}')\n",
    "print(f'   Locations: {insitu[\"location_name\"].nunique()} unique')\n",
    "print(f'   Columns: {list(insitu.columns)}')\n",
    "display(insitu.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1C: Satellite data (all CSVs combined) ───\n",
    "sat_files = sorted((DATA_ROOT / 'raw' / 'satellite').glob('*.csv'))\n",
    "sat_dfs = []\n",
    "for f in sat_files:\n",
    "    df = pd.read_csv(f, parse_dates=['acquisition_date'])\n",
    "    df['source_file'] = f.name\n",
    "    sat_dfs.append(df)\n",
    "    print(f'   {f.name}: {len(df):,} rows')\n",
    "\n",
    "satellite = pd.concat(sat_dfs, ignore_index=True)\n",
    "print(f'\\n3. satellite combined: {satellite.shape[0]:,} rows x {satellite.shape[1]} cols')\n",
    "print(f'   Date range: {satellite[\"acquisition_date\"].min()} → {satellite[\"acquisition_date\"].max()}')\n",
    "print(f'   Lakes: {satellite[\"lake_name\"].nunique()} → {satellite[\"lake_name\"].value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1D: Lake JSON data (has bloom labels!) ───\n",
    "lake_json_files = sorted((DATA_ROOT / 'raw' / 'lake_data').glob('*.json'))\n",
    "lake_records = []\n",
    "for f in lake_json_files:\n",
    "    if 'current_week' not in str(f):\n",
    "        with open(f) as fh:\n",
    "            data = json.load(fh)\n",
    "            for rec in data:\n",
    "                rec['source_file'] = f.name\n",
    "            lake_records.extend(data)\n",
    "            print(f'   {f.name}: {len(data):,} records')\n",
    "\n",
    "# Flatten nested JSON into a dataframe\n",
    "lake_flat = []\n",
    "for rec in lake_records:\n",
    "    flat = {\n",
    "        'date': rec.get('date'),\n",
    "        'lake': rec.get('lake'),\n",
    "        'source': rec.get('source'),\n",
    "        'quality_score': rec.get('quality_score'),\n",
    "        'source_file': rec.get('source_file'),\n",
    "    }\n",
    "    for k, v in rec.get('parameters', {}).items():\n",
    "        flat[f'param_{k}'] = v\n",
    "    for k, v in rec.get('habs_indicators', {}).items():\n",
    "        flat[f'hab_{k}'] = v\n",
    "    lake_flat.append(flat)\n",
    "\n",
    "lake_data = pd.DataFrame(lake_flat)\n",
    "lake_data['date'] = pd.to_datetime(lake_data['date'])\n",
    "\n",
    "print(f'\\n4. lake_json combined: {lake_data.shape[0]:,} rows')\n",
    "print(f'   Date range: {lake_data[\"date\"].min()} → {lake_data[\"date\"].max()}')\n",
    "print(f'   Lakes: {lake_data[\"lake\"].value_counts().to_dict()}')\n",
    "print(f'   Bloom statuses: {lake_data[\"hab_bloom_status\"].value_counts().to_dict()}')\n",
    "print(f'   Bloom probability: mean={lake_data[\"hab_bloom_probability\"].mean():.3f}, '\n",
    "      f'std={lake_data[\"hab_bloom_probability\"].std():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Quality Audit\n",
    "Flag synthetic data, zeros, missing values, suspicious uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2A: ML-ready data audit ───\n",
    "print('=' * 70)\n",
    "print('ML-READY DATA QUALITY AUDIT')\n",
    "print('=' * 70)\n",
    "\n",
    "feature_cols = ['chlorophyll_a', 'water_temperature', 'turbidity', 'dissolved_oxygen',\n",
    "                'ph', 'total_nitrogen', 'total_phosphorus', 'solar_radiation',\n",
    "                'wind_speed', 'precipitation']\n",
    "\n",
    "audit_rows = []\n",
    "for col in feature_cols:\n",
    "    n = len(ml_data)\n",
    "    zeros = (ml_data[col] == 0).sum()\n",
    "    nulls = ml_data[col].isna().sum()\n",
    "    unique = ml_data[col].nunique()\n",
    "    mean_val = ml_data[col].mean()\n",
    "    std_val = ml_data[col].std()\n",
    "    flag = []\n",
    "    if zeros / n > 0.5: flag.append('HIGH_ZEROS')\n",
    "    if nulls / n > 0.3: flag.append('HIGH_NULLS')\n",
    "    if std_val < 0.001: flag.append('NO_VARIANCE')\n",
    "    if unique < 5: flag.append('LOW_CARDINALITY')\n",
    "    audit_rows.append({\n",
    "        'feature': col, 'zeros': zeros, 'zero_pct': f'{zeros/n*100:.1f}%',\n",
    "        'nulls': nulls, 'unique': unique,\n",
    "        'mean': f'{mean_val:.3f}', 'std': f'{std_val:.3f}',\n",
    "        'flags': ', '.join(flag) if flag else 'OK'\n",
    "    })\n",
    "\n",
    "audit_df = pd.DataFrame(audit_rows)\n",
    "display(audit_df)\n",
    "\n",
    "# Lake name issues\n",
    "print(f'\\nLake column issues:')\n",
    "print(f'  \"object\" as lake name: {(ml_data[\"lake\"] == \"object\").sum()} rows')\n",
    "print(f'  Empty/null lake: {ml_data[\"lake\"].isna().sum()} rows')\n",
    "print(f'  Valid lakes: {ml_data[ml_data[\"lake\"] != \"object\"][\"lake\"].unique().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2B: In-situ data audit ───\n",
    "print('=' * 70)\n",
    "print('IN-SITU DATA QUALITY AUDIT')\n",
    "print('=' * 70)\n",
    "\n",
    "insitu_features = ['chlorophyll_a', 'turbidity', 'dissolved_oxygen', 'ph',\n",
    "                   'conductivity', 'temperature', 'wind_speed', 'air_temperature', 'humidity']\n",
    "\n",
    "audit_rows2 = []\n",
    "for col in insitu_features:\n",
    "    if col in insitu.columns:\n",
    "        n = len(insitu)\n",
    "        zeros = (insitu[col] == 0).sum()\n",
    "        nulls = insitu[col].isna().sum()\n",
    "        mn, mx = insitu[col].min(), insitu[col].max()\n",
    "        audit_rows2.append({\n",
    "            'feature': col, 'min': f'{mn:.2f}', 'max': f'{mx:.2f}',\n",
    "            'mean': f'{insitu[col].mean():.2f}', 'std': f'{insitu[col].std():.2f}',\n",
    "            'zeros': zeros, 'nulls': nulls\n",
    "        })\n",
    "\n",
    "display(pd.DataFrame(audit_rows2))\n",
    "print(f'\\nData sources: {insitu[\"data_source\"].value_counts().to_dict()}')\n",
    "print(f'Locations: {insitu[\"location_name\"].unique().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2C: Cross-source chlorophyll comparison ───\n",
    "fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "\n",
    "for ax, (name, series, color) in zip(axes, [\n",
    "    ('ml_ready', ml_data['chlorophyll_a'].dropna(), 'steelblue'),\n",
    "    ('in_situ', insitu['chlorophyll_a'].dropna(), 'coral'),\n",
    "    ('satellite', satellite['chlorophyll_index'].dropna(), 'seagreen'),\n",
    "    ('lake_json', lake_data['param_chlorophyll_a'].dropna(), 'orchid'),\n",
    "]):\n",
    "    ax.hist(series, bins=50, alpha=0.7, color=color, edgecolor='white')\n",
    "    ax.set_title(f'{name}\\nn={len(series):,}  '\n",
    "                 f'\\u03bc={series.mean():.2f}  \\u03c3={series.std():.2f}')\n",
    "    ax.set_xlabel('Chlorophyll (\\u03bcg/L or index)')\n",
    "    ax.axvline(series.mean(), color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle('Chlorophyll Distribution Across All Data Sources', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'fig1_chlorophyll_cross_source.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: figures/fig1_chlorophyll_cross_source.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Temporal Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ─── 3A: Records per lake per month (heatmaps) ───\nfig, axes = plt.subplots(2, 2, figsize=(20, 14))\n\ndatasets = [\n    ('ML-Ready', ml_data[ml_data['lake'] != 'object'].copy(), 'timestamp', 'lake'),\n    ('In-Situ', insitu.copy(), 'timestamp', 'location_name'),\n    ('Satellite', satellite.copy(), 'acquisition_date', 'lake_name'),\n    ('Lake JSON', lake_data.copy(), 'date', 'lake'),\n]\n\nfor ax, (title, df_copy, date_col, lake_col) in zip(axes.flat, datasets):\n    # Force datetime conversion (handles mixed timezone/format issues)\n    df_copy[date_col] = pd.to_datetime(df_copy[date_col], errors='coerce', utc=True)\n    df_copy = df_copy.dropna(subset=[date_col])\n    df_copy['yearmonth'] = df_copy[date_col].dt.to_period('M').astype(str)\n    pivot = df_copy.groupby([lake_col, 'yearmonth']).size().unstack(fill_value=0)\n    if pivot.shape[1] > 30:  # too many columns, sample\n        pivot = pivot.iloc[:, ::max(1, pivot.shape[1]//30)]\n    sns.heatmap(pivot, ax=ax, cmap='YlOrRd', cbar_kws={'label': 'Records'},\n                xticklabels=True, yticklabels=True)\n    ax.set_title(f'{title} ({len(df_copy):,} rows)', fontsize=12, fontweight='bold')\n    ax.tick_params(axis='x', rotation=90, labelsize=7)\n    ax.tick_params(axis='y', labelsize=8)\n\nplt.suptitle('Temporal Coverage: Records per Lake per Month', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig(RESULTS_DIR / 'figures' / 'fig2_temporal_coverage.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('Saved: figures/fig2_temporal_coverage.png')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Distributions & Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 4A: In-situ feature distributions (richest source) ───\n",
    "plot_features = ['chlorophyll_a', 'turbidity', 'dissolved_oxygen', 'ph',\n",
    "                 'temperature', 'conductivity', 'wind_speed', 'air_temperature', 'humidity']\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
    "for i, col in enumerate(plot_features):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    vals = insitu[col].dropna()\n",
    "    ax.hist(vals, bins=50, alpha=0.7, color='steelblue', edgecolor='white')\n",
    "    ax.axvline(vals.mean(), color='red', linestyle='--', alpha=0.7, label=f'mean={vals.mean():.1f}')\n",
    "    ax.axvline(vals.median(), color='orange', linestyle=':', alpha=0.7, label=f'median={vals.median():.1f}')\n",
    "    ax.set_title(f'{col}  (n={len(vals):,})', fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('In-Situ Feature Distributions', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'fig3_insitu_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 4B: Correlation matrices ───\n",
    "fig, axes = plt.subplots(1, 2, figsize=(22, 9))\n",
    "\n",
    "# In-situ\n",
    "corr1 = insitu[plot_features].corr()\n",
    "mask1 = np.triu(np.ones_like(corr1, dtype=bool))\n",
    "sns.heatmap(corr1, mask=mask1, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "            ax=axes[0], vmin=-1, vmax=1, square=True, linewidths=0.5)\n",
    "axes[0].set_title('In-Situ Feature Correlations', fontweight='bold')\n",
    "\n",
    "# ML-ready\n",
    "corr2 = ml_data[feature_cols].corr()\n",
    "mask2 = np.triu(np.ones_like(corr2, dtype=bool))\n",
    "sns.heatmap(corr2, mask=mask2, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "            ax=axes[1], vmin=-1, vmax=1, square=True, linewidths=0.5)\n",
    "axes[1].set_title('ML-Ready Feature Correlations', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Feature Correlation Comparison: In-Situ vs ML-Ready', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'fig4_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Bloom Label Analysis\n",
    "The lake JSON has `bloom_probability`, `bloom_status`, `cyanobacteria_density`, `toxin_levels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 5A: Bloom indicator statistics ───\n",
    "print('=' * 70)\n",
    "print('BLOOM INDICATOR ANALYSIS (from lake JSON monitoring data)')\n",
    "print('=' * 70)\n",
    "\n",
    "bloom_cols = ['hab_bloom_probability', 'hab_cyanobacteria_density', 'hab_toxin_levels']\n",
    "for col in bloom_cols:\n",
    "    print(f'\\n  {col}:')\n",
    "    print(f'    {lake_data[col].describe().to_dict()}')\n",
    "\n",
    "print(f'\\n  hab_bloom_status:')\n",
    "print(f'    {lake_data[\"hab_bloom_status\"].value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 5B: Bloom indicator visualizations ───\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "# Bloom probability distribution\n",
    "lake_data['hab_bloom_probability'].hist(bins=30, ax=axes[0, 0], color='crimson', alpha=0.7, edgecolor='white')\n",
    "axes[0, 0].axvline(0.5, color='black', linestyle='--', label='threshold=0.5')\n",
    "axes[0, 0].axvline(0.3, color='gray', linestyle=':', label='threshold=0.3')\n",
    "axes[0, 0].set_title('Bloom Probability Distribution', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Bloom status counts\n",
    "status_order = ['None', 'Low', 'Moderate', 'High', 'Critical']\n",
    "status_counts = lake_data['hab_bloom_status'].value_counts().reindex(status_order).fillna(0)\n",
    "colors_status = ['#2ecc71', '#f1c40f', '#e67e22', '#e74c3c', '#8e44ad']\n",
    "status_counts.plot.bar(ax=axes[0, 1], color=colors_status[:len(status_counts)], edgecolor='white')\n",
    "axes[0, 1].set_title('Bloom Status Counts', fontweight='bold')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Cyanobacteria density\n",
    "lake_data['hab_cyanobacteria_density'].hist(bins=30, ax=axes[0, 2], color='seagreen', alpha=0.7, edgecolor='white')\n",
    "axes[0, 2].set_title('Cyanobacteria Density', fontweight='bold')\n",
    "\n",
    "# Bloom probability by lake\n",
    "lake_data.boxplot(column='hab_bloom_probability', by='lake', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Bloom Probability by Lake', fontweight='bold')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "fig.suptitle('')\n",
    "\n",
    "# Chlorophyll vs bloom probability\n",
    "axes[1, 1].scatter(lake_data['param_chlorophyll_a'], lake_data['hab_bloom_probability'],\n",
    "                   alpha=0.3, s=10, c='steelblue')\n",
    "axes[1, 1].set_xlabel('Chlorophyll-a (\\u03bcg/L)')\n",
    "axes[1, 1].set_ylabel('Bloom Probability')\n",
    "axes[1, 1].set_title('Chlorophyll-a vs Bloom Probability', fontweight='bold')\n",
    "\n",
    "# Temperature vs bloom probability\n",
    "axes[1, 2].scatter(lake_data['param_temperature'], lake_data['hab_bloom_probability'],\n",
    "                   alpha=0.3, s=10, c='coral')\n",
    "axes[1, 2].set_xlabel('Temperature (\\u00b0C)')\n",
    "axes[1, 2].set_ylabel('Bloom Probability')\n",
    "axes[1, 2].set_title('Temperature vs Bloom Probability', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Bloom Label Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'fig5_bloom_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 5C: Compare labeling strategies ───\n",
    "print('=' * 70)\n",
    "print('LABELING STRATEGY COMPARISON')\n",
    "print('=' * 70)\n",
    "\n",
    "strategies = {\n",
    "    'bloom_prob >= 0.5': (lake_data['hab_bloom_probability'] >= 0.5).astype(int),\n",
    "    'bloom_prob >= 0.3': (lake_data['hab_bloom_probability'] >= 0.3).astype(int),\n",
    "    'status in [High,Critical]': lake_data['hab_bloom_status'].isin(['High', 'Critical']).astype(int),\n",
    "    'status in [Mod,High,Crit]': lake_data['hab_bloom_status'].isin(['Moderate', 'High', 'Critical']).astype(int),\n",
    "    'chlorophyll_a > 20': (lake_data['param_chlorophyll_a'] > 20).astype(int),\n",
    "    'chlorophyll_a > 10': (lake_data['param_chlorophyll_a'] > 10).astype(int),\n",
    "}\n",
    "\n",
    "print(f'{\"Strategy\":<30s} {\"Positive\":>10s} {\"Total\":>8s} {\"Rate\":>8s} {\"Imbalance\":>12s}')\n",
    "print('-' * 70)\n",
    "for name, labels in strategies.items():\n",
    "    pos = labels.sum()\n",
    "    total = len(labels)\n",
    "    ratio = f'1:{total // max(pos, 1)}'\n",
    "    print(f'{name:<30s} {pos:>10,} {total:>8,} {pos/total*100:>7.1f}% {ratio:>12s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Build Unified Research Dataset\n",
    "Merge in-situ (features) + satellite (spectral) + lake_json (bloom labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6A: Prepare in-situ as primary feature source ───\n",
    "research_insitu = insitu[['location_name', 'timestamp', 'latitude', 'longitude', 'station_id',\n",
    "                           'chlorophyll_a', 'turbidity', 'dissolved_oxygen', 'ph',\n",
    "                           'temperature', 'conductivity', 'wind_speed', 'air_temperature',\n",
    "                           'humidity', 'quality_score', 'data_source']].copy()\n",
    "\n",
    "# Standardize lake names (\"Bodensee - Überlingen\" → \"Bodensee\")\n",
    "research_insitu['lake'] = research_insitu['location_name'].str.split(' - ').str[0].str.strip()\n",
    "research_insitu['date'] = research_insitu['timestamp'].dt.normalize()\n",
    "\n",
    "print(f'In-situ prepared: {len(research_insitu):,} rows')\n",
    "print(f'Lakes: {research_insitu[\"lake\"].value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6B: Aggregate to daily per lake ───\n",
    "daily_insitu = research_insitu.groupby(['lake', 'date']).agg({\n",
    "    'chlorophyll_a': 'mean',\n",
    "    'turbidity': 'mean',\n",
    "    'dissolved_oxygen': 'mean',\n",
    "    'ph': 'mean',\n",
    "    'temperature': 'mean',\n",
    "    'conductivity': 'mean',\n",
    "    'wind_speed': 'mean',\n",
    "    'air_temperature': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'latitude': 'first',\n",
    "    'longitude': 'first',\n",
    "    'quality_score': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "daily_insitu['date'] = pd.to_datetime(daily_insitu['date'])\n",
    "print(f'Daily in-situ: {len(daily_insitu):,} rows')\n",
    "print(f'Per lake: {daily_insitu.groupby(\"lake\").size().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6C: Prepare satellite features (daily per lake) ───\n",
    "sat_features = satellite[['lake_name', 'acquisition_date', 'ndvi', 'surface_temperature',\n",
    "                           'chlorophyll_index', 'turbidity_index', 'cloud_coverage',\n",
    "                           'reflectance_blue', 'reflectance_green', 'reflectance_red',\n",
    "                           'reflectance_nir']].copy()\n",
    "sat_features = sat_features.rename(columns={'lake_name': 'lake', 'acquisition_date': 'date'})\n",
    "sat_features['date'] = sat_features['date'].dt.normalize()\n",
    "\n",
    "daily_sat = sat_features.groupby(['lake', 'date']).mean(numeric_only=True).reset_index()\n",
    "print(f'Daily satellite: {len(daily_sat):,} rows')\n",
    "print(f'Per lake: {daily_sat.groupby(\"lake\").size().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6D: Prepare bloom labels (daily per lake) ───\n",
    "lake_labels = lake_data[['date', 'lake', 'hab_bloom_probability', 'hab_bloom_status',\n",
    "                          'hab_cyanobacteria_density', 'hab_toxin_levels']].copy()\n",
    "lake_labels['date'] = lake_labels['date'].dt.normalize()\n",
    "\n",
    "daily_labels = lake_labels.groupby(['lake', 'date']).agg({\n",
    "    'hab_bloom_probability': 'mean',\n",
    "    'hab_cyanobacteria_density': 'mean',\n",
    "    'hab_toxin_levels': 'mean',\n",
    "    'hab_bloom_status': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "print(f'Daily labels: {len(daily_labels):,} rows')\n",
    "print(f'Per lake: {daily_labels.groupby(\"lake\").size().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6E: Merge all three sources ───\n",
    "# Strategy: in-situ as base → merge_asof satellite (±3 days) → merge_asof labels (±7 days)\n",
    "\n",
    "research_df = daily_insitu.sort_values(['lake', 'date']).copy()\n",
    "\n",
    "# Merge satellite\n",
    "merged_parts = []\n",
    "for lake_name in research_df['lake'].unique():\n",
    "    left = research_df[research_df['lake'] == lake_name].copy()\n",
    "    right = daily_sat[daily_sat['lake'] == lake_name].sort_values('date').copy()\n",
    "    if len(right) > 0:\n",
    "        merged = pd.merge_asof(left.sort_values('date'), right.drop(columns=['lake']),\n",
    "                               on='date', tolerance=pd.Timedelta('3D'), direction='nearest')\n",
    "    else:\n",
    "        merged = left\n",
    "    merged_parts.append(merged)\n",
    "research_df = pd.concat(merged_parts, ignore_index=True)\n",
    "\n",
    "# Merge bloom labels\n",
    "merged_parts2 = []\n",
    "for lake_name in research_df['lake'].unique():\n",
    "    left = research_df[research_df['lake'] == lake_name].sort_values('date').copy()\n",
    "    right = daily_labels[daily_labels['lake'] == lake_name].sort_values('date').copy()\n",
    "    if len(right) > 0:\n",
    "        merged = pd.merge_asof(left, right.drop(columns=['lake']),\n",
    "                               on='date', tolerance=pd.Timedelta('7D'), direction='nearest')\n",
    "    else:\n",
    "        merged = left\n",
    "    merged_parts2.append(merged)\n",
    "research_df = pd.concat(merged_parts2, ignore_index=True)\n",
    "\n",
    "print(f'\\n{\"=\" * 70}')\n",
    "print(f'UNIFIED RESEARCH DATASET')\n",
    "print(f'{\"=\" * 70}')\n",
    "print(f'Shape: {research_df.shape}')\n",
    "print(f'Date range: {research_df[\"date\"].min()} → {research_df[\"date\"].max()}')\n",
    "print(f'Lakes: {research_df[\"lake\"].value_counts().to_dict()}')\n",
    "print(f'\\nColumns ({len(research_df.columns)}): {list(research_df.columns)}')\n",
    "print(f'\\nNull counts (top 10):')\n",
    "display(research_df.isnull().sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Create Bloom Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 7A: Multi-criteria bloom label ───\n",
    "def create_bloom_label(row, threshold=0.5):\n",
    "    \"\"\"Priority: bloom_probability → chlorophyll+temp threshold → 0\"\"\"\n",
    "    if pd.notna(row.get('hab_bloom_probability')):\n",
    "        return int(row['hab_bloom_probability'] >= threshold)\n",
    "    chl = row.get('chlorophyll_a', np.nan)\n",
    "    temp = row.get('temperature', np.nan)\n",
    "    if pd.notna(chl):\n",
    "        if chl > 20: return 1\n",
    "        if chl > 10 and pd.notna(temp) and temp > 18: return 1\n",
    "    return 0\n",
    "\n",
    "research_df['bloom_label'] = research_df.apply(create_bloom_label, axis=1)\n",
    "\n",
    "# Continuous risk score for regression\n",
    "research_df['bloom_risk'] = research_df['hab_bloom_probability'].fillna(\n",
    "    research_df['chlorophyll_a'].clip(0, 50) / 50\n",
    ")\n",
    "\n",
    "pos = research_df['bloom_label'].sum()\n",
    "total = len(research_df)\n",
    "print(f'Binary label: {pos} positive / {total} total ({pos/total*100:.1f}%)')\n",
    "print(f'Continuous risk: mean={research_df[\"bloom_risk\"].mean():.3f}, std={research_df[\"bloom_risk\"].std():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 7B: Label quality visualization ───\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "research_df['bloom_label'].value_counts().sort_index().plot.bar(\n",
    "    ax=axes[0], color=['steelblue', 'crimson'], edgecolor='white')\n",
    "axes[0].set_title('Binary Label Balance', fontweight='bold')\n",
    "axes[0].set_xticklabels(['No Bloom (0)', 'Bloom (1)'], rotation=0)\n",
    "for i, v in enumerate(research_df['bloom_label'].value_counts().sort_index()):\n",
    "    axes[0].text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "research_df['bloom_risk'].hist(bins=30, ax=axes[1], color='orchid', alpha=0.7, edgecolor='white')\n",
    "axes[1].axvline(0.5, color='black', linestyle='--', label='threshold=0.5')\n",
    "axes[1].set_title('Continuous Bloom Risk', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "bloom_by_lake = research_df.groupby('lake')['bloom_label'].mean().sort_values(ascending=False)\n",
    "bloom_by_lake.plot.bar(ax=axes[2], color='seagreen', edgecolor='white')\n",
    "axes[2].set_title('Bloom Rate by Lake', fontweight='bold')\n",
    "axes[2].set_ylabel('Fraction with bloom=1')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Label Quality Assessment', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'fig6_label_quality.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Temporal Train / Val / Test Split\n",
    "**70% train / 15% val / 15% test — temporal, NOT random**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 8A: Split ───\n",
    "research_df = research_df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "date_min = research_df['date'].min()\n",
    "date_max = research_df['date'].max()\n",
    "date_range = (date_max - date_min).days\n",
    "\n",
    "train_end = date_min + pd.Timedelta(days=int(date_range * 0.70))\n",
    "val_end   = date_min + pd.Timedelta(days=int(date_range * 0.85))\n",
    "\n",
    "train = research_df[research_df['date'] <= train_end].copy()\n",
    "val   = research_df[(research_df['date'] > train_end) & (research_df['date'] <= val_end)].copy()\n",
    "test  = research_df[research_df['date'] > val_end].copy()\n",
    "\n",
    "print(f'Date range: {date_min.date()} → {date_max.date()} ({date_range} days)')\n",
    "print(f'\\n{\"Split\":<8s} {\"Rows\":>8s} {\"From\":>14s} {\"To\":>14s} {\"Bloom %\":>10s}')\n",
    "print('-' * 60)\n",
    "for name, df in [('Train', train), ('Val', val), ('Test', test)]:\n",
    "    print(f'{name:<8s} {len(df):>8,} {str(df[\"date\"].min().date()):>14s} '\n",
    "          f'{str(df[\"date\"].max().date()):>14s} {df[\"bloom_label\"].mean()*100:>9.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 8B: Visualize split ───\n",
    "fig, ax = plt.subplots(figsize=(18, 4))\n",
    "\n",
    "for i, (name, df, color) in enumerate([\n",
    "    ('Train', train, 'steelblue'), ('Val', val, 'orange'), ('Test', test, 'crimson')\n",
    "]):\n",
    "    ax.scatter(df['date'], [i]*len(df), alpha=0.4, s=8, c=color, label=f'{name} ({len(df):,})')\n",
    "\n",
    "ax.axvline(train_end, color='black', linestyle='--', alpha=0.5, label=f'train_end={train_end.date()}')\n",
    "ax.axvline(val_end, color='black', linestyle=':', alpha=0.5, label=f'val_end={val_end.date()}')\n",
    "ax.set_yticks([0, 1, 2])\n",
    "ax.set_yticklabels(['Train', 'Val', 'Test'])\n",
    "ax.set_title('Temporal Train / Val / Test Split', fontweight='bold')\n",
    "ax.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'fig7_temporal_split.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Monolithic Baselines (RQ1)\n",
    "Train GradientBoosting + RandomForest + Ensemble on ALL features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 9A: Define feature sets ───\n",
    "INSITU_FEATURES = ['chlorophyll_a', 'turbidity', 'dissolved_oxygen', 'ph',\n",
    "                   'temperature', 'conductivity', 'wind_speed', 'air_temperature', 'humidity']\n",
    "\n",
    "SATELLITE_FEATURES = ['ndvi', 'surface_temperature', 'chlorophyll_index',\n",
    "                      'turbidity_index', 'cloud_coverage']\n",
    "\n",
    "ALL_FEATURES = INSITU_FEATURES + SATELLITE_FEATURES\n",
    "TARGET = 'bloom_label'\n",
    "\n",
    "# Use only features that exist in the merged dataset\n",
    "available_features = [f for f in ALL_FEATURES if f in research_df.columns]\n",
    "missing_features = [f for f in ALL_FEATURES if f not in research_df.columns]\n",
    "print(f'Available features ({len(available_features)}): {available_features}')\n",
    "if missing_features:\n",
    "    print(f'Missing features ({len(missing_features)}): {missing_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 9B: Prepare X, y ───\n",
    "def prepare_split(df, features, target):\n",
    "    X = df[features].copy()\n",
    "    y = df[target].copy()\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_split(train, available_features, TARGET)\n",
    "X_val, y_val = prepare_split(val, available_features, TARGET)\n",
    "X_test, y_test = prepare_split(test, available_features, TARGET)\n",
    "\n",
    "# Impute missing values with median, then standardize\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_proc = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "X_val_proc = scaler.transform(imputer.transform(X_val))\n",
    "X_test_proc = scaler.transform(imputer.transform(X_test))\n",
    "\n",
    "print(f'Train: X={X_train_proc.shape}, y={y_train.shape} (bloom={y_train.mean():.3f})')\n",
    "print(f'Val:   X={X_val_proc.shape}, y={y_val.shape} (bloom={y_val.mean():.3f})')\n",
    "print(f'Test:  X={X_test_proc.shape}, y={y_test.shape} (bloom={y_test.mean():.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 9C: Train models ───\n",
    "print('Training models...')\n",
    "\n",
    "models = OrderedDict()\n",
    "\n",
    "models['GradientBoosting'] = GradientBoostingClassifier(\n",
    "    n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "    subsample=0.8, min_samples_leaf=10, random_state=42\n",
    ")\n",
    "models['RandomForest'] = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=10, min_samples_leaf=5,\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "for name, model in models.items():\n",
    "    t0 = datetime.now()\n",
    "    model.fit(X_train_proc, y_train)\n",
    "    elapsed = (datetime.now() - t0).total_seconds()\n",
    "    print(f'  {name}: trained in {elapsed:.1f}s')\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 9D: Evaluate all models ───\n",
    "def evaluate(name, y_true, y_prob):\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    n_classes = len(np.unique(y_true))\n",
    "    return {\n",
    "        'model': name,\n",
    "        'AUROC': roc_auc_score(y_true, y_prob) if n_classes > 1 else np.nan,\n",
    "        'AUPRC': average_precision_score(y_true, y_prob) if n_classes > 1 else np.nan,\n",
    "        'Brier': brier_score_loss(y_true, y_prob),\n",
    "        'Accuracy': (y_pred == y_true).mean(),\n",
    "        'TP': ((y_pred == 1) & (y_true == 1)).sum(),\n",
    "        'FP': ((y_pred == 1) & (y_true == 0)).sum(),\n",
    "        'FN': ((y_pred == 0) & (y_true == 1)).sum(),\n",
    "        'TN': ((y_pred == 0) & (y_true == 0)).sum(),\n",
    "    }\n",
    "\n",
    "# Validation results\n",
    "print('VALIDATION SET RESULTS')\n",
    "print('=' * 80)\n",
    "val_results = []\n",
    "ens_prob_val = np.zeros(len(y_val))\n",
    "ens_prob_test = np.zeros(len(y_test))\n",
    "\n",
    "for name, model in models.items():\n",
    "    prob_val = model.predict_proba(X_val_proc)[:, 1]\n",
    "    prob_test = model.predict_proba(X_test_proc)[:, 1]\n",
    "    ens_prob_val += prob_val / len(models)\n",
    "    ens_prob_test += prob_test / len(models)\n",
    "    val_results.append(evaluate(name, y_val, prob_val))\n",
    "\n",
    "val_results.append(evaluate('Ensemble (GB+RF)', y_val, ens_prob_val))\n",
    "val_df = pd.DataFrame(val_results)\n",
    "display(val_df)\n",
    "\n",
    "# Test results\n",
    "print('\\nTEST SET RESULTS (Final)')\n",
    "print('=' * 80)\n",
    "test_results = []\n",
    "for name, model in models.items():\n",
    "    prob = model.predict_proba(X_test_proc)[:, 1]\n",
    "    test_results.append(evaluate(name, y_test, prob))\n",
    "test_results.append(evaluate('Ensemble (GB+RF)', y_test, ens_prob_test))\n",
    "test_df = pd.DataFrame(test_results)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 9E: ROC, PR curves, Feature Importance, Calibration ───\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# ROC\n",
    "ax = axes[0, 0]\n",
    "for name, model in models.items():\n",
    "    prob = model.predict_proba(X_test_proc)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, prob)\n",
    "    auc = roc_auc_score(y_test, prob)\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})', linewidth=2)\n",
    "fpr_e, tpr_e, _ = roc_curve(y_test, ens_prob_test)\n",
    "auc_e = roc_auc_score(y_test, ens_prob_test)\n",
    "ax.plot(fpr_e, tpr_e, '--', label=f'Ensemble (AUC={auc_e:.3f})', linewidth=2)\n",
    "ax.plot([0, 1], [0, 1], 'k:', alpha=0.3)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve (Test Set)', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# PR\n",
    "ax = axes[0, 1]\n",
    "for name, model in models.items():\n",
    "    prob = model.predict_proba(X_test_proc)[:, 1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, prob)\n",
    "    ap = average_precision_score(y_test, prob)\n",
    "    ax.plot(rec, prec, label=f'{name} (AP={ap:.3f})', linewidth=2)\n",
    "prec_e, rec_e, _ = precision_recall_curve(y_test, ens_prob_test)\n",
    "ap_e = average_precision_score(y_test, ens_prob_test)\n",
    "ax.plot(rec_e, prec_e, '--', label=f'Ensemble (AP={ap_e:.3f})', linewidth=2)\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall Curve (Test Set)', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Feature importance\n",
    "ax = axes[1, 0]\n",
    "gb_imp = models['GradientBoosting'].feature_importances_\n",
    "rf_imp = models['RandomForest'].feature_importances_\n",
    "idx = np.argsort(gb_imp)[::-1]\n",
    "x_pos = np.arange(len(available_features))\n",
    "ax.barh(x_pos - 0.2, gb_imp[idx], 0.4, label='GradientBoosting', color='steelblue', alpha=0.8)\n",
    "ax.barh(x_pos + 0.2, rf_imp[idx], 0.4, label='RandomForest', color='coral', alpha=0.8)\n",
    "ax.set_yticks(x_pos)\n",
    "ax.set_yticklabels([available_features[i] for i in idx])\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Feature Importance Comparison', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Calibration\n",
    "ax = axes[1, 1]\n",
    "for name, prob in [('GB', models['GradientBoosting'].predict_proba(X_test_proc)[:, 1]),\n",
    "                   ('RF', models['RandomForest'].predict_proba(X_test_proc)[:, 1]),\n",
    "                   ('Ensemble', ens_prob_test)]:\n",
    "    try:\n",
    "        fraction_pos, mean_pred = calibration_curve(y_test, prob, n_bins=10, strategy='uniform')\n",
    "        ax.plot(mean_pred, fraction_pos, 's-', label=name, linewidth=2, markersize=6)\n",
    "    except ValueError:\n",
    "        pass\n",
    "ax.plot([0, 1], [0, 1], 'k:', alpha=0.3, label='Perfect calibration')\n",
    "ax.set_xlabel('Mean Predicted Probability')\n",
    "ax.set_ylabel('Fraction of Positives')\n",
    "ax.set_title('Calibration Plot (Test Set)', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle('RQ1 Monolithic Baseline — Full Evaluation', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'fig8_baseline_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 9F: Modality ablation (quick preview for RQ1) ───\n",
    "# What happens when we remove satellite OR in-situ features?\n",
    "print('=' * 70)\n",
    "print('MODALITY ABLATION (preview for RQ1 - graceful degradation)')\n",
    "print('=' * 70)\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "feature_sets = {\n",
    "    'All Features': available_features,\n",
    "    'In-Situ Only': [f for f in INSITU_FEATURES if f in available_features],\n",
    "    'Satellite Only': [f for f in SATELLITE_FEATURES if f in available_features],\n",
    "}\n",
    "\n",
    "for set_name, feats in feature_sets.items():\n",
    "    if len(feats) == 0:\n",
    "        print(f'  {set_name}: NO FEATURES AVAILABLE, skipping')\n",
    "        continue\n",
    "    \n",
    "    imp = SimpleImputer(strategy='median')\n",
    "    sc = StandardScaler()\n",
    "    X_tr = sc.fit_transform(imp.fit_transform(train[feats]))\n",
    "    X_te = sc.transform(imp.transform(test[feats]))\n",
    "    \n",
    "    gb_abl = GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "                                        subsample=0.8, random_state=42)\n",
    "    gb_abl.fit(X_tr, y_train)\n",
    "    prob = gb_abl.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    m = evaluate(set_name, y_test, prob)\n",
    "    m['n_features'] = len(feats)\n",
    "    m['features'] = ', '.join(feats)\n",
    "    ablation_results.append(m)\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_results)[['model', 'n_features', 'AUROC', 'AUPRC', 'Brier', 'Accuracy']]\n",
    "display(ablation_df)\n",
    "\n",
    "print('\\n→ This shows how much each modality contributes.')\n",
    "print('  In RQ1, we compare this monolithic ablation to the agentic architecture.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save Everything & Export ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 10A: Save research dataset ───\n",
    "research_df.to_parquet(RESULTS_DIR / 'data' / 'unified_research_dataset.parquet', index=False)\n",
    "train.to_parquet(RESULTS_DIR / 'data' / 'train.parquet', index=False)\n",
    "val.to_parquet(RESULTS_DIR / 'data' / 'val.parquet', index=False)\n",
    "test.to_parquet(RESULTS_DIR / 'data' / 'test.parquet', index=False)\n",
    "\n",
    "# Save models\n",
    "for name, model in models.items():\n",
    "    with open(RESULTS_DIR / 'models' / f'baseline_{name.lower()}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "with open(RESULTS_DIR / 'models' / 'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open(RESULTS_DIR / 'models' / 'imputer.pkl', 'wb') as f:\n",
    "    pickle.dump(imputer, f)\n",
    "\n",
    "# Save results tables\n",
    "val_df.to_csv(RESULTS_DIR / 'val_results.csv', index=False)\n",
    "test_df.to_csv(RESULTS_DIR / 'test_results.csv', index=False)\n",
    "ablation_df.to_csv(RESULTS_DIR / 'ablation_results.csv', index=False)\n",
    "audit_df.to_csv(RESULTS_DIR / 'data_audit.csv', index=False)\n",
    "\n",
    "# Save experiment metadata\n",
    "experiment_meta = {\n",
    "    'experiment': 'RQ1_Phase1_Monolithic_Baselines',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'dataset': {\n",
    "        'total_records': len(research_df),\n",
    "        'train': {'records': len(train), 'date_range': [str(train['date'].min()), str(train['date'].max())], 'bloom_rate': float(y_train.mean())},\n",
    "        'val': {'records': len(val), 'date_range': [str(val['date'].min()), str(val['date'].max())], 'bloom_rate': float(y_val.mean())},\n",
    "        'test': {'records': len(test), 'date_range': [str(test['date'].min()), str(test['date'].max())], 'bloom_rate': float(y_test.mean())},\n",
    "        'lakes': research_df['lake'].unique().tolist(),\n",
    "        'n_lakes': int(research_df['lake'].nunique()),\n",
    "        'features_used': available_features,\n",
    "        'n_features': len(available_features),\n",
    "    },\n",
    "    'models': {\n",
    "        'GradientBoosting': {'n_estimators': 200, 'max_depth': 5, 'lr': 0.1},\n",
    "        'RandomForest': {'n_estimators': 200, 'max_depth': 10},\n",
    "    },\n",
    "    'test_results': test_df.to_dict(orient='records'),\n",
    "    'ablation_results': ablation_df.to_dict(orient='records'),\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'experiment_metadata.json', 'w') as f:\n",
    "    json.dump(experiment_meta, f, indent=2, default=str)\n",
    "\n",
    "print('All results saved to results/ directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 10B: Create ZIP of all results ───\n",
    "zip_path = 'swim_research_phase1_results.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    for root, dirs, files in os.walk(RESULTS_DIR):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, '.')\n",
    "            zf.write(file_path, arcname)\n",
    "\n",
    "zip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
    "print(f'\\nResults ZIP created: {zip_path} ({zip_size_mb:.1f} MB)')\n",
    "print(f'\\nContents:')\n",
    "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "    for info in zf.infolist():\n",
    "        print(f'  {info.filename:<55s} {info.file_size/1024:>8.1f} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 10C: Download ZIP (Colab only) ───\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(zip_path)\n",
    "    print('Download started!')\n",
    "else:\n",
    "    print(f'Results zip ready at: {os.path.abspath(zip_path)}')\n",
    "    print('Copy it from your Jupyter file browser.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Next Steps\n",
    "\n",
    "### What we did:\n",
    "1. Loaded & audited 4 data sources (~110K total records)\n",
    "2. Built unified research dataset (in-situ + satellite + bloom labels)\n",
    "3. Created multi-criteria bloom labels\n",
    "4. Temporal train/val/test split\n",
    "5. Trained monolithic baselines (GB, RF, Ensemble)\n",
    "6. Evaluated: AUROC, AUPRC, Brier, Accuracy, Calibration\n",
    "7. Modality ablation (in-situ only vs satellite only vs all)\n",
    "8. Exported everything as ZIP\n",
    "\n",
    "### Results saved in ZIP:\n",
    "```\n",
    "results/\n",
    "  figures/          (8 publication-quality plots)\n",
    "  models/           (GB, RF, scaler, imputer pickles)\n",
    "  data/             (train/val/test parquets)\n",
    "  val_results.csv\n",
    "  test_results.csv\n",
    "  ablation_results.csv\n",
    "  data_audit.csv\n",
    "  experiment_metadata.json\n",
    "```\n",
    "\n",
    "### Next notebooks:\n",
    "- **02**: Deep learning baselines (LSTM, Transformer) — needs GPU\n",
    "- **03**: Agentic model comparison (run same data through SWIM agents)\n",
    "- **04**: Communication protocol experiments (RQ2)\n",
    "- **05**: Conflict resolution & fusion experiments (RQ3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}